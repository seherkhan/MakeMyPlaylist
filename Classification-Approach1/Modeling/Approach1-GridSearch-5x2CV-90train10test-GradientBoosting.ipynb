{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# datafile has no duplicates (not a multilabelling problem)\n",
    "datafile='/home/seherkhan/myfiles/coursework/usc/fall2018/MLforDI/project/FinalDraft/data/playlist_data_21oct_withoutdups.csv'\n",
    "df=pd.read_csv(datafile,sep='|')\n",
    "tmp_X = df.iloc[:,4:18]\n",
    "tmp_y = df.iloc[:,18:19]\n",
    "tmp_y['playlist'] = tmp_y['playlist'].astype('category')\n",
    "tmp_y['playlist_codes']=tmp_y['playlist'].cat.codes\n",
    "scaler = StandardScaler()\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X=scaler.fit_transform(tmp_X)\n",
    "y=np.ravel(tmp_y['playlist_codes'])\n",
    "\n",
    "X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial model\n",
    "kf = KFold(n_splits=2,shuffle=True)\n",
    "    \n",
    "for train_index,test_index in kf.split(X,y):\n",
    "    X_1,X_2=X[train_index],X[test_index]\n",
    "    y_1,y_2=y[train_index],y[test_index]\n",
    "        \n",
    "    X_train = X_1\n",
    "    X_test = X_2\n",
    "    y_train = y_1\n",
    "    y_test = y_2\n",
    "\n",
    "gb=GradientBoostingClassifier()\n",
    "gb.fit(X_train,y_train)\n",
    "pred = gb.predict(X_test)\n",
    "score = precision_recall_fscore_support(y_test, pred, labels=range(14), average='micro')\n",
    "accuracy = accuracy_score(y_test, pred, normalize=True)\n",
    "prediction = score[0]\n",
    "recall = score[1]\n",
    "f_score = score[2]\n",
    "\n",
    "print 'accuracy =', accuracy\n",
    "print 'prediction =', prediction\n",
    "print 'recall =', recall\n",
    "print 'f_score =', f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune n_estimators\n",
    "param_grid_nest={'n_estimators':range(20,150,20)}\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.1, max_features='sqrt', subsample=0.8, random_state=10)\n",
    "gs1 = GridSearchCV(estimator = estimator, param_grid = param_grid_nest,n_jobs=4)\n",
    "gs1.fit(X,y)\n",
    "print gs1.best_params_\n",
    "print gs1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune max_depth and min_samples_split\n",
    "param_grid_dp_sampsplit = {'max_depth':range(5,10,2), 'min_samples_split':range(200,500,50)}\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=40, max_features='sqrt', subsample=0.8, random_state=10)\n",
    "gs2 = GridSearchCV(estimator = estimator, param_grid = param_grid_dp_sampsplit,n_jobs=4)\n",
    "gs2.fit(X,y)\n",
    "print gs2.best_params_\n",
    "print gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune min_samples_leaf\n",
    "param_grid_sampleaf = {'min_samples_leaf':range(80,200,40),'max_depth':range(5,13,2)}\n",
    "estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=40, min_samples_split=350, max_features='sqrt', subsample=0.8, random_state=10)\n",
    "gs2 = GridSearchCV(estimator = estimator, param_grid = param_grid_sampleaf,n_jobs=4)\n",
    "gs2.fit(X,y)\n",
    "print gs2.best_params_\n",
    "print gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "#Continue with 5x2CV\n",
    "\n",
    "min_samples_split = 350 \n",
    "min_samples_leaf = 100\n",
    "max_depth = 7\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "\n",
    "print 'Gradient Boosting: performance on training data 5*2 CV'\n",
    "gb=GradientBoostingClassifier(min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf,\n",
    "                            max_depth = max_depth, max_features =max_features,subsample=subsample)\n",
    "\n",
    "acc=[]\n",
    "pre=[]\n",
    "re=[]\n",
    "f_sc=[]\n",
    "\n",
    "accuracy = 0\n",
    "prediction = 0 \n",
    "recall = 0\n",
    "f_score = 0\n",
    "\n",
    "for n in range(5):\n",
    "    kf = KFold(n_splits=2,shuffle=True)\n",
    "    \n",
    "    for train_index,test_index in kf.split(X_trainset,y_trainset):\n",
    "        X_1,X_2=X[train_index],X[test_index]\n",
    "        y_1,y_2=y[train_index],y[test_index]\n",
    "        \n",
    "    X_train = X_1\n",
    "    X_val = X_2\n",
    "    y_train = y_1\n",
    "    y_val = y_2\n",
    "    \n",
    "    gb.fit(X_train, y_train)\n",
    "    pred = gb.predict(X_val)\n",
    "    score = precision_recall_fscore_support(y_val, pred, labels=range(14), average='micro')\n",
    "    accuracy = accuracy + accuracy_score(y_val, pred, normalize=True)\n",
    "    prediction = prediction + score[0]\n",
    "    recall = recall + score[1]\n",
    "    f_score = f_score + score[2]\n",
    "    acc.append(accuracy_score(y_val, pred, normalize=True))\n",
    "    pre.append(score[0])\n",
    "    re.append(score[1])\n",
    "    f_sc.append(score[2])\n",
    "    \n",
    "    X_train = X_2\n",
    "    X_val = X_1\n",
    "    y_train = y_2\n",
    "    y_val = y_1\n",
    "    \n",
    "    gb.fit(X_train, y_train)\n",
    "    pred = gb.predict(X_val)\n",
    "    score = precision_recall_fscore_support(y_val, pred, labels=range(14), average='micro')\n",
    "    accuracy = accuracy + accuracy_score(y_val, pred, normalize=True)\n",
    "    prediction = prediction + score[0]\n",
    "    recall = recall + score[1]\n",
    "    f_score = f_score + score[2]\n",
    "    acc.append(accuracy_score(y_val, pred, normalize=True))\n",
    "    pre.append(score[0])\n",
    "    re.append(score[1])\n",
    "    f_sc.append(score[2])\n",
    "    \n",
    "# calculate average accuracy\n",
    "accuracy = accuracy/10\n",
    "prediction = prediction/10 \n",
    "recall = recall/10\n",
    "f_score = f_score/10\n",
    "\n",
    "print 'accuracy =', accuracy,\"+-\",1.96*np.std(acc)/math.sqrt(10)\n",
    "print 'prediction =', prediction,\"+-\",1.96*np.std(pre)/math.sqrt(10)\n",
    "print 'recall =', recall,\"+-\",1.96*np.std(re)/math.sqrt(10)\n",
    "print 'f_score =', f_score,\"+-\",1.96*np.std(f_sc)/math.sqrt(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Gradient Boosting: performance on test data of model fitted to training data in one go'\n",
    "\n",
    "min_samples_split = 350 \n",
    "min_samples_leaf = 100\n",
    "max_depth = 7\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "\n",
    "gb1=GradientBoostingClassifier(min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf,\n",
    "                            max_depth = max_depth, max_features =max_features,subsample=subsample)\n",
    "gb1.fit(X_trainset,y_trainset)\n",
    "pred = gb1.predict(X_testset)\n",
    "score_1 = precision_recall_fscore_support(y_testset, pred, labels=range(14), average='micro')\n",
    "print 'accuracy =', accuracy_score(y_testset, pred, normalize=True)\n",
    "print 'prediction =', score_1[0]\n",
    "print 'recall =', score_1[1]\n",
    "print 'f_score =', score_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "print gb1.feature_importances_\n",
    "\n",
    "# Relative feature importance according to the model gb1 (fitted on training data)\n",
    "rel_importance = gb1.feature_importances_/gb1.feature_importances_.max()\n",
    "pyplot.figure(figsize=(8, 4)) \n",
    "pyplot.bar(tmp_X.keys(),rel_importance)\n",
    "pyplot.xticks(rotation='vertical')\n",
    "pyplot.ylabel('Relative Feature Importance')\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
